{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-spokesman",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exempt-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-selling",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "clear-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SepalLength  SepalWidth  PetalLength  PetalWidth         Name\n",
       "0           5.1         3.5          1.4         0.2  Iris-setosa\n",
       "1           4.9         3.0          1.4         0.2  Iris-setosa\n",
       "2           4.7         3.2          1.3         0.2  Iris-setosa\n",
       "3           4.6         3.1          1.5         0.2  Iris-setosa\n",
       "4           5.0         3.6          1.4         0.2  Iris-setosa\n",
       "5           5.4         3.9          1.7         0.4  Iris-setosa\n",
       "6           4.6         3.4          1.4         0.3  Iris-setosa\n",
       "7           5.0         3.4          1.5         0.2  Iris-setosa\n",
       "8           4.4         2.9          1.4         0.2  Iris-setosa\n",
       "9           4.9         3.1          1.5         0.1  Iris-setosa\n",
       "10          5.4         3.7          1.5         0.2  Iris-setosa\n",
       "11          4.8         3.4          1.6         0.2  Iris-setosa\n",
       "12          4.8         3.0          1.4         0.1  Iris-setosa\n",
       "13          4.3         3.0          1.1         0.1  Iris-setosa\n",
       "14          5.8         4.0          1.2         0.2  Iris-setosa"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.github.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/iris.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "critical-texture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "count   150.000000  150.000000   150.000000  150.000000\n",
       "mean      5.843333    3.054000     3.758667    1.198667\n",
       "std       0.828066    0.433594     1.764420    0.763161\n",
       "min       4.300000    2.000000     1.000000    0.100000\n",
       "25%       5.100000    2.800000     1.600000    0.300000\n",
       "50%       5.800000    3.000000     4.350000    1.300000\n",
       "75%       6.400000    3.300000     5.100000    1.800000\n",
       "max       7.900000    4.400000     6.900000    2.500000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-truck",
   "metadata": {},
   "source": [
    "The features of this dataset will be the measurements of the flowers which are the first 3 columns. Once the flower's measurement is plotted this data will show which flower these types of measurements belong to. The type of flower will be the label. The mapping of the labels will classify which flower family the measurement belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-outreach",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "chronic-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.9,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-setup",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "turned-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "log_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-eleven",
   "metadata": {},
   "source": [
    "This score for the logistic regression means that logistic regression is a really good way to classify which type of flower we are looking at. Logistic Regression seems to be a very good type of model to use whe are trying to use classify something for a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "noted-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.17258812e-06 3.12626456e-02 9.68736182e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(log_reg.predict_proba(X_test.head(1)))\n",
    "log_reg.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-plane",
   "metadata": {},
   "source": [
    "For the first dataset in the X_test dataset, you can see that this data point has the highest chance to be identified as a viriginica flower type. The chances of this datapoint being classified as the other 2 are way lower than the first class. The third class has a 96% while the second has around a 3% and the first one has a super low chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "wrong-roommate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48402288,  0.85004545, -2.38910086, -1.05089957],\n",
       "       [ 0.56021955, -0.47252684, -0.15961602, -0.98763573],\n",
       "       [-0.07619667, -0.37751861,  2.54871688,  2.0385353 ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "buried-jenny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.07268986,   2.31651617, -12.38920603])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-singapore",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "piano-madagascar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel = 'linear', probability = True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "disturbed-greensboro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0027849 , 0.00486525, 0.99234984]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.predict_proba(X_test.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-soldier",
   "metadata": {},
   "source": [
    "After the model was fitted with the training dataset, it shows that the testing score comes out to be 100%. This means that when a this model guesses one of these three flowers it will always be correct. So far it seems like the SVM model does the best a multiple feature classifications, beating the Logisic Regression model by 6% using the same datasets.\n",
    "The probability that the first datapoint in X_test will be guessed the third flower (virginica) is at 99%. This model is very confident at classfiying this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-efficiency",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "expanded-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xe717491af0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encode = LabelEncoder()\n",
    "y_trainN = encode.fit_transform(y_train)\n",
    "y_testN = encode.fit_transform(y_test)\n",
    "\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(20, activation = 'relu', input_shape = (4,)))\n",
    "nn_model.add(Dense(10, activation = 'relu'))\n",
    "nn_model.add(Dense(1, activation = 'softmax'))\n",
    "nn_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "nn_model.fit(X_train, y_trainN, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "stupid-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 987us/step - loss: 0.0000e+00 - accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "test_acc = nn_model.evaluate(X_test,y_testN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "decimal-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xe71b2ed4f0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(100, activation = 'relu', input_shape = (4,)))\n",
    "nn_model.add(Dense(10, activation = 'relu'))\n",
    "nn_model.add(Dense(1, activation = 'softmax'))\n",
    "nn_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "nn_model.fit(X_train, y_trainN, epochs=50, batch_size = 4, verbose =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "minus-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x000000E71B7A8040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "test_acc = nn_model.evaluate(X_test,y_testN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-council",
   "metadata": {},
   "source": [
    "The Neural Networks seem to be not very good at finding the correct classficaion of the iris dataset. This neural networks accruacy score seems to only come out at around 33% which is a lot less than the other models. Having more hidden layers and more epochs did not seem to bring the model to predict better and stayed at 33% across all of the epochs which is kind of weird. The highest score achieved was 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-webmaster",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "independent-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ruled-strengthening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.predict_proba(X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "handy-property",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-vancouver",
   "metadata": {},
   "source": [
    "This score means that when the K-Nearets Neighbor algorithm was looking for datapoints around this first datapoint in the X_test dataset. It was found that the 3 other datapoints close to the current one maps to the third class which is iris-virignica. The other 2 classes did not have any datapoints close to the first datapoint of the X_test dataset. The KNN porobability does not function the same as the other models because KNN is just trying to map the current datapoint to its closest datapoints to predict which class this datapoint belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-stuff",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-imaging",
   "metadata": {},
   "source": [
    "From all of the models above it seems that the Support Vector Machine was one of the most successful at accurately classfying a datapoint that was have no been trained yet. The SVM model prodicted all of the data in the X_test dataset correctly and matched exactly to the results shown in the y_test. The other models were also really close. The Logistic Regression model was getting a score of around 93% and this is already really great with a minor errors, but people would probably want a little possible error as possile in their mchine learning model, so this would not be the model to use. \n",
    "The Neural Network Models were not as good as the rest of models, so it would not be recommended to use this model, this does not mean that this model is bad and could probably be used way better at a different kind of dataset.\n",
    "The KNN model was also super good at predicting the datapoints too, probably because it is using datapoints around itself to guess the current datapoint. The KNN model also had a 100% accuray as predicting the class of the flowers in the test data. This model is as useful as the SVM model and could be also used to help people classify the type of flowers in this datset without any errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
